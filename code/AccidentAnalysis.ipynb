{"cells":[{"cell_type":"markdown","source":["###Prediction of Accident prone locations using Apache Spark on Databricks Community Edition\n###Problem Statement\nEvery day a number of people die out of road accidents all over the world. The severity of road accidents is more in densely populated countries. A country's asset is their population and the health and safety of it is the top priority of every country. This project would help in decreasing road accidents and ensure better road safety. We haved used the UK road accidents dataset to analyse and classify which locations are more prone to severe accidents. We have taken into consideration numerous attributes which affects road accidents to predict the same. Additionally we will be plotting these accidents prone locations on Google map depending on the severity of accidents that took place at those locations.\nIn order to implement this project we have used python and Apache Spark.\n\n\n###Dataset\n1. <a href=\"https://github.com/kavyaprasad/accident_Data/blob/master/DfTRoadSafety_Accidents_2015.csv\"><b>AccidentData.csv</b></a> - This file contails data about Accident. The various attributes present in the dataset are\n  1. Accident Index: Index of each accident (STRING)\n  2. Location Easting OSGR: Gives Us the exact location of the place (INTEGER)\n  3. Location Northing OSGR: Gives Us the exact location of the place (INTEGER)\n  4. Longitude : Longitude of the place (INTEGER)\n  5. Latitude: Latitude of the place (INTEGER)\n  6. Police Force: Number of police available at that time (INTEGER)\n  7. Accident Severity:  Severity of the accident (INTEGER)\n  8. Number of Vehicles:  Number of involved in the accident (INTEGER)\n  9. Number of Casualties:  Number of people died or injured (INTEGER)\n  10. Date: The date of travel (INTEGER)\n  11. Day of Week: Day of the week (INTEGER)\n  12. Time: Time of the accident (STRING)\n  13. Local Authority(District): local district number (INTEGER)\n  14. Local Authority(Highway): local Highway number (STRING)\n  15. 1st Road Class: Hierarchy which the road falls in  (INTEGER)\n  16. 1st Road Number: Road number (STRING)\n  17. Speed limit: Speed limit in the road (INTEGER)\n  18. Junction Detail: Each number in this column various junction types (INTEGER)\n  19. Junction Control: Kind of control (INTEGER)\n  20. 2nd Road Class: Hierarchy which the road falls in (INTEGER)\n  21. 2nd Road Number: Road number (STRING)\n  22. Pedestrian Crossing Human Control: Human Control for Pedestrian Crossing (INTEGER)\n  23. Pedestrian Crossing Physical Facilities: Physical Facilities for Pedestrian Crossing (INTEGER)\n  24. Light Conditions: Daytime or Nightime (INTEGER)\n  25. Weather Conditions: Weather Conditions when the accident happened (INTEGER)\n  26. Road Surface Conditions: Road Surface Conditions where the accident occured (INTEGER)\n  27. Special Conditions at Site: Other Conditions prevailing at the accident spot (INTEGER)\n  28. Carriageway Hazards:  Carriageway Hazards in the past (STRING)\n  29. Urban or Rural Area: Is it an Urban or rural area (INTEGER)\n  30. Did Police Officer Attend Scene of Accident: Did Police Officer Attend Scene of Accident (INTEGER)\n  31. LSOA of Accident Location: Accident location (STRING)"],"metadata":{}},{"cell_type":"markdown","source":["### Dataset Link\n\nDownload the dataset using the shell command wget and the URL, save them into the tmp directory. The URL for the dataset is\nAccidentData: https://raw.githubusercontent.com/kavyaprasad/accident_Data/master/DfTRoadSafety_Accidents_2015.csv"],"metadata":{}},{"cell_type":"code","source":["%sh\nwget -P /tmp \"https://raw.githubusercontent.com/kavyaprasad/accident_Data/master/DfTRoadSafety_Accidents_2015.csv\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["### Uploading the dataset into Databricks file system\n\nDatabricks file system is a distributed file system lying on top of Amazon S3. We will upload the data from the local file system into our DBFS. Below is a python script which copies the data from the local file system into the datasets folder of DBFS of your cluster.\n\nThe local files are referenced using `file:/` and DBFS files are referenced using `dbfs:/`"],"metadata":{}},{"cell_type":"code","source":["localAccidentDataFilePath = \"file:/tmp/DfTRoadSafety_Accidents_2015.csv\"\ndbutils.fs.mkdirs(\"dbfs:/datasets\")\ndbutils.fs.cp(localAccidentDataFilePath, \"dbfs:/datasets/\")\n#Displaying the files present in the DBFS datasets folder of your cluser\ndisplay(dbutils.fs.ls(\"dbfs:/datasets\"))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#Number of accidents based on the severity\nAccidentRDD=sc.textFile(\"dbfs:/datasets/DfTRoadSafety_Accidents_2015.csv\")\nheader= AccidentRDD.first()\nAccidentFilterRDD = AccidentRDD.filter(lambda line:line!=header)\nAccidentSplitRDD = AccidentFilterRDD.map(lambda row: row.split(\",\"))\nAccidentMapRDD2=AccidentSplitRDD.map(lambda x : (x[6],1)).reduceByKey(lambda acc, val : acc+val)\nAccidentSwappedRDD2=AccidentMapRDD2.map(lambda (a, b): (b, a))\nAccidentDescendingRDD2=AccidentSwappedRDD2.sortByKey(0)\nfor i in AccidentDescendingRDD2.take(20): print(i)\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#Number of accidents and their severity during daytime\nnew_AccidentSplitRDD=AccidentSplitRDD.filter(lambda x : x[24]==\"1\")\nAccidentMapRDD2=new_AccidentSplitRDD.map(lambda x : (x[6],1)).reduceByKey(lambda acc, val : acc+val)\nAccidentSwappedRDD2=AccidentMapRDD2.map(lambda (a, b): (b, a))\nAccidentDescendingRDD2=AccidentSwappedRDD2.sortByKey(0)\nfor i in AccidentDescendingRDD2.take(20): print(i)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#Number of accidents and their severity during night time\n\nnew_AccidentSplitRDD3=AccidentSplitRDD.filter(lambda x : x[24]==\"4\")\nAccidentMapRDD3=new_AccidentSplitRDD3.map(lambda x : (x[6],1)).reduceByKey(lambda acc, val : acc+val)\nAccidentSwappedRDD3=AccidentMapRDD3.map(lambda (a, b): (b, a))\nAccidentDescendingRDD3=AccidentSwappedRDD3.sortByKey(0)\nfor i in AccidentDescendingRDD3.take(20): print(i)\n\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.classification import LogisticRegression, OneVsRest\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.sql.types import *\nfrom pyspark.sql import SparkSession\nfrom collections import namedtuple\nfrom pyspark.sql import SQLContext, Row\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\nfrom pyspark.ml import Pipeline\nfrom pylab import *"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# printing data\nfor i in AccidentSplitRDD.take(20): print(i)\n\n#Created schema for the temporary table to be created\nparsedData = AccidentSplitRDD.map(lambda p: Row(Accident_Index=p[0],Location_Easting_OSGR=p[1],Location_Northing_OSGR=p[2],Longitude=p[3],Latitude=p[4],Police_Force=p[5],Accident_Severity=p[6],Number_of_Vehicles=p[7],Number_of_Casualties=p[8],Date=p[9],Day_of_Week=p[10],Time=p[11],Local_Authority_District=p[12],Local_Authority_Highway=p[13],first_Road_Class=p[14],first_Road_Number=p[15],Road_Type=p[16],Speed_limit=p[17],Junction_Detail=p[18],Junction_Control=p[19],second_Road_Class=p[20],second_Road_Number=p[21],Pedestrian_Crossing_Human_Control=p[22],Pedestrian_Crossing_Physical_Facilities=p[23],Light_Conditions=p[24],Weather_Conditions=p[25],Road_Surface_Conditions=p[26],Special_Conditions_at_Site=p[27],Carriageway_Hazards=p[28],Urban_or_Rural_Area=p[29],Did_Police_Officer_Attend_Scene_of_Accident=p[30],LSOA_of_Accident_Location=p[31]))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#Dataframe is created from the parsed data\nschemaAllData = spark.createDataFrame(parsedData)#dataframe is created\nschemaAllData.printSchema()# schema of the dataframe printed\nschemaAllData.show()\nschemaAllData=schemaAllData.na.drop()#dropped rows containing null values \nschemaAllData.registerTempTable(\"AccidentData\")#the dataframe is registered to a temporary table named AccidentData\n\n# COMMAND ----------\nsql_resultsall = sqlContext.sql(\"Select Accident_Index,Police_Force,Accident_Severity,Number_of_Vehicles,Number_of_Casualties,Day_of_Week,first_Road_Class,Road_Type,Speed_limit,Junction_Detail,Junction_Control,second_Road_Class,Pedestrian_Crossing_Human_Control,Pedestrian_Crossing_Physical_Facilities,Light_Conditions,Weather_Conditions,Road_Surface_Conditions,Special_Conditions_at_Site,Carriageway_Hazards,Urban_or_Rural_Area from AccidentData\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#sql commands for transforming time\n\nsql_results = sqlContext.sql(\"Select Accident_Index,Time from AccidentData where Time>='18:00' and Time <='23:59'\")\nsql_results1 = sqlContext.sql(\"Select Accident_Index,Time from AccidentData where Time>='12:00' and Time <='17:59'\")\nsql_results2 = sqlContext.sql(\"Select Accident_Index,Time from AccidentData where Time>='06:00' and Time <='11:59'\")\nsql_results3 = sqlContext.sql(\"Select Accident_Index,Time from AccidentData where Time>='00:00' and Time <='05:59'\")\n\n# Time Datframes converted to rdd and transformed\nsql_results3 = sql_results3.rdd\ntime3 = sql_results3.map(lambda p: (p[0],1))# Time between '00:00' and '05:59' is transformed to 1\nsql_results2 = sql_results2.rdd\ntime2 = sql_results2.map(lambda p: (p[0],2))# Time between '06:00' and '11:59' is transformed to 2\nsql_results1 = sql_results1.rdd\ntime1 = sql_results1.map(lambda p: (p[0],3))# Time between '12:00' and '17:59' is transformed to 3\nsql_results = sql_results.rdd\ntime = sql_results.map(lambda p: (p[0],4))# Time between '18:00' and '23:59' is transformed to 4\n\ntimeRDD=time3.union(time2).union(time1).union(time)# created union of all 4 time rdd\ntimeData=timeRDD.map(lambda p: Row(Accident_Index1=p[0],Time1=p[1]))# created schema\n\nschemaNewTime = spark.createDataFrame(timeData)\nschemaNewTime.printSchema()\nschemaNewTime.show()\nschemaNewTime.registerTempTable(\"timeData\")\nsql_TimeResults = sqlContext.sql(\"Select Accident_Index1,Time1 from timeData\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Join operation performed\n\njoin = sqlContext.sql(\"select Accident_Index,Accident_Index1,Police_Force,Accident_Severity as binary_response,Number_of_Vehicles,Number_of_Casualties,Day_of_Week,first_Road_Class,Road_Type,Speed_limit,Junction_Detail,Junction_Control,second_Road_Class,Pedestrian_Crossing_Human_Control,Pedestrian_Crossing_Physical_Facilities,Light_Conditions,Weather_Conditions,Road_Surface_Conditions,Special_Conditions_at_Site,Carriageway_Hazards,Urban_or_Rural_Area,Time1 from timeData,AccidentData where timeData.Accident_Index1 = AccidentData.Accident_Index and AccidentData.Accident_Index IS NOT NULL and timeData.Accident_Index1 IS NOT NULL \")\n\n# All data values are casted into float values \njoinrdd=join.select(join.Police_Force.cast('float'),join.binary_response.cast('float'),join.Number_of_Vehicles.cast('float'),join.Number_of_Casualties.cast('float'),join.Day_of_Week.cast('float'),join.first_Road_Class.cast('float'),join.Road_Type.cast('float'),join.Speed_limit.cast('float'),join.Junction_Detail.cast('float'),join.Junction_Control.cast('float'),join.second_Road_Class.cast('float'),join.Pedestrian_Crossing_Human_Control.cast('float'),join.Pedestrian_Crossing_Physical_Facilities.cast('float'),join.Light_Conditions.cast('float'),join.Weather_Conditions.cast('float'),join.Road_Surface_Conditions.cast('float'),join.Special_Conditions_at_Site.cast('float'),join.Carriageway_Hazards.cast('float'),join.Urban_or_Rural_Area.cast('float'),join.Time1.cast('float'))\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#This section of code is refered from:\n#cite:https://vanishingcodes.wordpress.com/2016/06/09/pyspark-tutorial-building-a-random-forest-binary-classifier-on-unbalanced-dataset/\ncols_now =['Police_Force','Number_of_Vehicles','Number_of_Casualties','Day_of_Week','first_Road_Class','Road_Type','Speed_limit','Junction_Detail','Junction_Control','second_Road_Class','Pedestrian_Crossing_Human_Control','Pedestrian_Crossing_Physical_Facilities','Light_Conditions','Weather_Conditions','Road_Surface_Conditions','Special_Conditions_at_Site','Carriageway_Hazards','Urban_or_Rural_Area','Time1']\ncols_now1=['Police_Force1','Number_of_Vehicles1','Number_of_Casualties1','Day_of_Week1']\n\nindexers = [StringIndexer(inputCol=x, outputCol=x+'_tmp')\n            for x in cols_now ]\n\n\nencoders = [OneHotEncoder(dropLast=False, inputCol=x+\"_tmp\", outputCol=y)\nfor x,y in zip(cols_now,cols_now1)]\ntmp = [[i,j] for i,j in zip(indexers, encoders)]\ntmp = [i for sublist in tmp for i in sublist]\n\n\nassembler_features = VectorAssembler(inputCols=cols_now, outputCol='features')\nlabelIndexer = StringIndexer(inputCol='binary_response', outputCol='label')\ntmp += [assembler_features, labelIndexer]\npipeline = Pipeline(stages=tmp)\n\nallData = pipeline.fit(joinrdd).transform(joinrdd)\nallData.cache()\ntrainingData, testData = allData.randomSplit([0.8,0.2], seed=0) "],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# COMMAND ----------\n# Decision Tree\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\nmodel = dt.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error percentage: %\"+str((1.0 - accuracy)*100))\nprint(\"Accuracy percentage: %\"+str(accuracy*100))\n\n# summary only\nprint(model)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# COMMAND ----------\n#Logistic Regression\nlr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n\n# instantiate the One Vs Rest Classifier.\novr = OneVsRest(classifier=lr)\n\n# train the multiclass model.\novrModel = ovr.fit(trainingData)\n\n# score the model on test data.\npredictions = ovrModel.transform(testData)\n\n# obtain evaluator.\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n\n# compute the classification error on test data.\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error percentage: %\" +str((1 - accuracy)*100))\nprint(\"Accuracy percentage: %\" + str(accuracy*100))"],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"accident","notebookId":1677533169833609},"nbformat":4,"nbformat_minor":0}
